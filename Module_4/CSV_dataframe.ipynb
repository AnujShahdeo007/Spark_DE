{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c187f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSv created at: ./messy.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os,tempfile,shutil\n",
    "\n",
    "spark=(SparkSession.builder\n",
    "    .appName(\"CSV Options\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"Error\")\n",
    "\n",
    "#create a temp folder \n",
    "\n",
    "base_dir=tempfile.mkdtemp()\n",
    "csv_path=\"./messy.csv\"\n",
    "\n",
    "data=\"\"\"id|name|city|amount|comment\n",
    "1|\"Amit\"|\"Mumbai\"|100.5|\"Normal row\n",
    "2|\"Riya\"|\"New york, USA\"|200.0|\"City contains comma\"\n",
    "3|\"John\"|\"Delhi\"|NULL|\"AMount id NULL string -> should become null \"\n",
    "4|\"Sara\"|\"Kolakata\"|300.75|\"HE said \\\\\"Hello\\\\\" to me\"\n",
    "5|\"Gaurav\"|\"Bangalore\"|400.0|\"This is multiline comments\n",
    "second line of comments\"\n",
    "6|\"Badrow\"|\"chennai\"|500.0\n",
    "7|\"Extracol\"|\"pune\"|600.0|\"ok\"|\"extra\"\n",
    "\"\"\"\n",
    "\n",
    "with open(csv_path,\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(data)\n",
    "print(\"CSv created at:\",csv_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a679e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anujshahdeo/Documents/GithubRepos/Spark_DE/Module_4\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48cf47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV \n",
    "df1=spark.read.format(\"csv\").load(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6915cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|_c0                                                                 |\n",
      "+--------------------------------------------------------------------+\n",
      "|id|name|city|amount|comment                                         |\n",
      "|1|\"Amit\"|\"Mumbai\"|100.5|\"Normal row                                 |\n",
      "|2|\"Riya\"|\"New york                                                  |\n",
      "|3|\"John\"|\"Delhi\"|NULL|\"AMount id NULL string -> should become null \"|\n",
      "|4|\"Sara\"|\"Kolakata\"|300.75|\"HE said \\\"Hello\\\" to me\"                |\n",
      "|5|\"Gaurav\"|\"Bangalore\"|400.0|\"This is multiline comments            |\n",
      "|second line of comments\"                                            |\n",
      "|6|\"Badrow\"|\"chennai\"|500.0                                          |\n",
      "|7|\"Extracol\"|\"pune\"|600.0|\"ok\"|\"extra\"                              |\n",
      "+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c965c2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d269e",
   "metadata": {},
   "source": [
    "    - columns will be c0,c1\n",
    "    - everthing is string \n",
    "        - | won't be trated as seprator correctly (if expects comma by default)\n",
    "    - Multiple row will break badly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e87ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 \n",
    "# Read csv correct way \n",
    "\n",
    "df2= (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\",\"true\") # columns \n",
    "        .option(\"sep\",\"|\") # delimieter\n",
    "        .load(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d725c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "|id                      |name    |city         |amount|comment                                     |\n",
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "|1                       |Amit    |Mumbai       |100.5 |Normal row                                  |\n",
      "|2                       |Riya    |New york, USA|200.0 |City contains comma                         |\n",
      "|3                       |John    |Delhi        |NULL  |AMount id NULL string -> should become null |\n",
      "|4                       |Sara    |Kolakata     |300.75|HE said \"Hello\" to me                       |\n",
      "|5                       |Gaurav  |Bangalore    |400.0 |This is multiline comments                  |\n",
      "|second line of comments\"|NULL    |NULL         |NULL  |NULL                                        |\n",
      "|6                       |Badrow  |chennai      |500.0 |NULL                                        |\n",
      "|7                       |Extracol|pune         |600.0 |ok                                          |\n",
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11488b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52a219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3 Enable inferschema (demo/ not prod)\n",
    "\n",
    "df3= (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\",\"true\") # columns \n",
    "        .option(\"sep\",\"|\") # delimieter\n",
    "        .option(\"inferSchema\",\"true\") # automatically guess datatype \n",
    "        .load(csv_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f52dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "|id                      |name    |city         |amount|comment                                     |\n",
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "|1                       |Amit    |Mumbai       |100.5 |Normal row                                  |\n",
      "|2                       |Riya    |New york, USA|200.0 |City contains comma                         |\n",
      "|3                       |John    |Delhi        |NULL  |AMount id NULL string -> should become null |\n",
      "|4                       |Sara    |Kolakata     |300.75|HE said \"Hello\" to me                       |\n",
      "|5                       |Gaurav  |Bangalore    |400.0 |This is multiline comments                  |\n",
      "|second line of comments\"|NULL    |NULL         |NULL  |NULL                                        |\n",
      "|6                       |Badrow  |chennai      |500.0 |NULL                                        |\n",
      "|7                       |Extracol|pune         |600.0 |ok                                          |\n",
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4953e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aab5a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 Use nullvalue so NULL become actual null\n",
    "\n",
    "df4= (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\",\"true\") # columns \n",
    "        .option(\"sep\",\"|\") # delimieter\n",
    "        .option(\"inferSchema\",\"true\") # automatically guess datatype \n",
    "        .option(\"nullValue\",\"NULL\")\n",
    "        .load(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a572a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "|id                      |name    |city         |amount|comment                                     |\n",
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "|1                       |Amit    |Mumbai       |100.5 |Normal row                                  |\n",
      "|2                       |Riya    |New york, USA|200.0 |City contains comma                         |\n",
      "|3                       |John    |Delhi        |NULL  |AMount id NULL string -> should become null |\n",
      "|4                       |Sara    |Kolakata     |300.75|HE said \"Hello\" to me                       |\n",
      "|5                       |Gaurav  |Bangalore    |400.0 |This is multiline comments                  |\n",
      "|second line of comments\"|NULL    |NULL         |NULL  |NULL                                        |\n",
      "|6                       |Badrow  |chennai      |500.0 |NULL                                        |\n",
      "|7                       |Extracol|pune         |600.0 |ok                                          |\n",
      "+------------------------+--------+-------------+------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724945a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e310df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5= (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\",\"true\") # columns \n",
    "        .option(\"sep\",\"|\") # delimieter\n",
    "        .option(\"inferSchema\",\"true\") # automatically guess datatype \n",
    "        .option(\"nullValue\",\"NULL\")\n",
    "        .option(\"quote\",'\"')\n",
    "        .option(\"escape\",\"\\\\\")\n",
    "        .option(\"multiLine\",\"True\")\n",
    "        .load(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bb305bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+------+---------------------------------------------------+\n",
      "|id |name    |city     |amount|comment                                            |\n",
      "+---+--------+---------+------+---------------------------------------------------+\n",
      "|1  |Amit    |Mumbai   |100.5 |\"Normal row\\n2|\"Riya\"                              |\n",
      "|3  |John    |Delhi    |NULL  |AMount id NULL string -> should become null        |\n",
      "|4  |Sara    |Kolakata |300.75|HE said \"Hello\" to me                              |\n",
      "|5  |Gaurav  |Bangalore|400.0 |This is multiline comments\\nsecond line of comments|\n",
      "|6  |Badrow  |chennai  |500.0 |NULL                                               |\n",
      "|7  |Extracol|pune     |600.0 |ok                                                 |\n",
      "+---+--------+---------+------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44e34802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6= (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\",\"true\") # columns \n",
    "        .option(\"sep\",\"|\") # delimieter\n",
    "        .option(\"inferSchema\",\"true\") # automatically guess datatype \n",
    "        .option(\"nullValue\",\"NULL\")\n",
    "        .option(\"quote\",'\"')\n",
    "        .option(\"escape\",\"\\\\\")\n",
    "        .option(\"multiLine\",\"True\")\n",
    "        .option(\"mode\",\"PERMISSIVE\")\n",
    "        .option(\"columnNameOfCorruptRecord\",\"_badrecords\")\n",
    "        .load(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e06e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+------+---------------------------------------------------+\n",
      "|id |name    |city     |amount|comment                                            |\n",
      "+---+--------+---------+------+---------------------------------------------------+\n",
      "|1  |Amit    |Mumbai   |100.5 |\"Normal row\\n2|\"Riya\"                              |\n",
      "|3  |John    |Delhi    |NULL  |AMount id NULL string -> should become null        |\n",
      "|4  |Sara    |Kolakata |300.75|HE said \"Hello\" to me                              |\n",
      "|5  |Gaurav  |Bangalore|400.0 |This is multiline comments\\nsecond line of comments|\n",
      "|6  |Badrow  |chennai  |500.0 |NULL                                               |\n",
      "|7  |Extracol|pune     |600.0 |ok                                                 |\n",
      "+---+--------+---------+------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a133cadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20dbefee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_badrecords` cannot be resolved. Did you mean one of the following? [`id`, `name`, `city`, `amount`, `comment`].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df6.filter(\u001b[43mdf6\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_badrecords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.isNotNull()).show(truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GithubRepos/Spark_DE/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3080\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   3008\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns the column as a :class:`Column`.\u001b[39;00m\n\u001b[32m   3009\u001b[39m \n\u001b[32m   3010\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3077\u001b[39m \u001b[33;03m+---+----+\u001b[39;00m\n\u001b[32m   3078\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3080\u001b[39m     jc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n\u001b[32m   3082\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Column):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GithubRepos/Spark_DE/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GithubRepos/Spark_DE/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `_badrecords` cannot be resolved. Did you mean one of the following? [`id`, `name`, `city`, `amount`, `comment`]."
     ]
    }
   ],
   "source": [
    "df6.filter(df6[\"_badrecords\"].isNotNull()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44e240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
